# Description:	This file contains the base class for lexers that use CLTK.

############
# Requires #
############

# Standard Library
require "string_scanner"

# Crystal Language Toolkit
require "./token"

#######################
# Classes and Modules #
#######################

module CLTK

  # A LexingError exception is raised when an input stream contains a
  # substring that isn't matched by any of a lexer's rules.
  class LexingError < Exception
    # @return [Integer]
    getter :stream_offset

    # @return [Integer]
    getter :line_number

    # @return [Integer]
    getter :line_offset

    # @return [String]
    getter :remainder

    # @param [Integer]	stream_offset	Offset from begnning of string.
    # @param [Integer]	line_number	Number of newlines encountered so far.
    # @param [Integer]	line_offset	Offset from beginning of line.
    # @param [String]	remainder		Rest of the string that couldn't be lexed.
    def initialize(@stream_offset : Int32, @line_number : Int32, @line_offset : Int32, @remainder : String)
      super(message)
      @backtrace = [] of String
    end

    # @return [String] String representation of the error.
    def to_s
      "#{super()}: #{@remainder}"
    end
  end

  # The Lexer class may be sub-classed to produce new lexers.  These lexers
  # have a lot of features, and are described in the main documentation.
  class Lexer

    # @return [Environment] Environment used by an instantiated lexer.
    getter :env

    #################
    # Class Methods #
    #################

    # @return [Symbol] State in which the lexer starts.
    getter :start_state

    def self.start_state
      @@start_state
    end

    def self.setenv(env)
      @@env = env
    end

    # Called when the Lexer class is sub-classed, it installes
    # necessary instance class variables.
    #
    # @return [void]
    macro inherited
      @@env = Environment
      @@match_type	= :longest
      @@rules		= {} of Symbol => Array(Rule)
      @@rules[:default] = [] of Rule
      @@start_state	= :default
    end

    # Lex *string*, using *env* as the environment.  This method will
    # return the array of tokens generated by the lexer with a token
    # of type EOS (End of Stream) appended to the end.
    #
    # @param [String]		string	String to be lexed.
    # @param [String]		file_name	File name used for recording token positions.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex(string, file_name = nil, env = @@env.new(@@start_state))
      # Offset from start of stream.
      stream_offset = 0

      # Offset from the start of the line.
      line_offset = 0
      line_number = 1

      # Empty token list.
      tokens = Array(Token).new

      # The scanner.
      scanner = StringScanner.new(string)

      # Start scanning the input string.
      until scanner.eos?
	match = nil
	# If the match_type is set to :longest all of the
	# rules for the current state need to be scanned
	# and the longest match returned.  If the
	# match_type is :first, we only need to scan until
	# we find a match.
	@@rules[env.state].each do |rule|
	  if (rule.flags - env.flags).empty?
	    if txt = scanner.check(rule.pattern)
	      if !match || match.first.size < txt.size
		match = {txt, rule}
		break if @@match_type == :first
	      end
	    end
	  end
	end
	if match
	  rule = match[1]
	  txt = scanner.scan(rule.pattern).not_nil!

#	  type, value = env.rule_exec(rule.pattern.match(txt.not_nil!), txt, )
          type, value = rule.action.call(rule.pattern.match(txt).not_nil!, txt, env)

	  if type
	    pos = StreamPosition.new(stream_offset, line_number, line_offset, txt.size, file_name)
	    tokens << Token.new(type, value, pos)
	  end

	  # Advance our stat counters.
	  stream_offset += txt.size

	  if (newlines = txt.count("\n")) > 0
	    line_number += newlines
	    line_offset = txt.split("\n").last.size
	  else
	    line_offset += txt.size()
	  end
	else
          raise LexingError.new(stream_offset, line_number, line_offset, scanner.rest)
	end
      end

      return tokens << Token.new(:EOS)
    end

    # A wrapper function that calls {Lexer.lex} on the contents of a
    # file.
    #
    # @param [String]		file_name	File to be lexed.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex_file(file_name, env = @@env.new(@start_state))
      File.open(file_name, 'r') { |f| self.lex(f.read, file_name, env) }
    end

    # Used to tell a lexer to use the first match found instead
    # of the longest match found.
    #
    # @return [void]
    def self.match_first
      @@match_type = :first
    end

    # This method is used to define a new lexing rule.  The
    # first argument is the regular expression used to match
    # substrings of the input.  The second argument is the state
    # to which the rule belongs.  Flags that need to be set for
    # the rule to be considered are specified by the third
    # argument.  The last argument is a block that returns a
    # type and value to be used in constructing a Token. If no
    # block is specified the matched substring will be
    # discarded and lexing will continue.
    #
    # @param [Regexp, String]	pattern	Pattern for matching text.
    # @param [Symbol]			state	State in which this rule is active.
    # @param [Array<Symbol>]		flags	Flags which must be set for rule to be active.
    # @param [Proc]			action	Proc object that produces Tokens.
    #
    # @return [void]

    alias BlockReturn = { Symbol, Nil } | {String, String} | { Nil, Nil } | { Symbol, Int32 } | {Symbol, Float64} | { Symbol, String } | { Symbol, Array(String) }

    macro rule(pattern, state = :default, flags = [] of Symbol, &action: _ -> _)
      {{pattern}}.tap do |pattern|
        Rule.new(
          (pattern.is_a?(String)) ? Regex.new(pattern) : pattern,
          {{state}},
          {{flags}}
        ) do |%match, %txt, %env|
            {% if action  %}
              {% if action.args.first %}
                {{action.args.first.id}} = %txt
              {% end %}
            %res = %env.yield_with_match(%match) do
              {{action.body}}
            end
            if %res.is_a? Void
              {nil, nil}
            elsif %res.is_a? Tuple
              %res
            else
              Tuple.new(%res, nil)
            end
            {% else %}
              {nil, nil}
            {% end %}
        end.tap do |rule|
          if {{state}} == :ALL
            @@rules.each_key { |k| @@rules[k] << rule }
          elsif @@rules[{{state}}]?
            @@rules[{{state}}] << rule
          else
            @@rules[{{state}}] = [ rule ]
          end
        end
      end
    end

    def self.rule(pattern, state = :default, flags = [] of String)
      self.rule(pattern, state, flags) do
        {nil, nil}
      end
    end

    # Changes the starting state of the lexer.
    #
    # @param [Symbol] state Starting state for this lexer.
    #
    # @return [void]
    def self.start(state)
      @@start_state = state
    end

    # All actions passed to LexerCore.rule are evaluated inside an
    # instance of the Environment class or its subclass (which must have
    # the same name).  This class provides functions for manipulating
    # lexer state and flags.
    # see http://carc.in/#/r/1i9
    class Environment
      # @return [Array<Symbol>] Flags currently set in this environment.
      getter :flags

      # @return [Match] Match object generated by a rule's regular expression.
      #property :match

      # Instantiates a new Environment object.
      #
      # @param [Symbol]	start_state	Lexer's start state.
      # @param [Match]	match		Match object for matching text.

      @state: Array(Symbol)
      @match: Regex::MatchData?

      def match
        @match as Regex::MatchData
      end

      macro method_missing(name, args, block)
        {% if name == "yield_with_match" %}
          @match = {{args.first}}.not_nil!
          with self as {{@type}} yield
        {% end%}
      end

      def initialize(start_state, @match = nil)
        @state	= [start_state]
        @flags	= [] of Symbol
      end

      # Pops a state from the state stack.
      #
      # @return [void]
      def pop_state
        @state.pop

        nil
      end

      # Pushes a new state onto the state stack.
      #
      # @return [void]
      def push_state(state)
        @state << state
        nil
      end

      # Sets the value on the top of the state stack.
      #
      # @param [Symbol] state New state for the lexing environment.
      #
      # @return [void]
      def set_state(state)
        @state[-1] = state
        nil
      end

      # @return [Symbol] Current state of the lexing environment.
      def state
        @state.last
      end

      # Sets a flag in the current environment.
      #
      # @param [Symbol] flag Flag to set as enabled.
      #
      # @return [void]
      def set_flag(flag)
        unless @flags.includes?(flag)
	  @flags << flag
        end
        nil
      end

      # Unsets a flag in the current environment.
      #
      # @param [Symbol] flag Flag to unset.
      #
      # @return [void]
      def unset_flag(flag)
        @flags.delete(flag)
        nil
      end

      # Unsets all flags in the current environment.
      #
      # @return [void]
      def clear_flags
        @flags = Array.new
        nil
      end
    end

    ####################
    # Instance Methods #
    ####################

    # Instantiates a new lexer and creates an environment to be
    # used for subsequent calls.

    @env: Environment

    def initialize
      @env = @@env.new(self.class.start_state)
    end

    # Lexes a string using the encapsulated environment.
    #
    # @param [String] string		String to be lexed.
    # @param [String] file_name	File name used for Token positions.
    #
    # @return [Array<Token>]
    def lex(string, file_name = nil)
      self.class.lex(string, file_name, @env)
    end

    # Lexes a file using the encapsulated environment.
    #
    # @param [String] file_name File to be lexed.
    #
    # @return [Array<Token>]
    def lex_file(file_name)
      self.class.lex_file(file_name, @env)
    end

    # The Rule class is used simply for data encapsulation.
    class Rule
      # @return [Proc] Token producting action to be taken when this rule is matched.
      getter :action

      # @return [Regexp] Regular expression for matching this rule.
      getter :pattern

      # @return [Array<Symbol>] Flags currently set in this lexing environment.
      getter :flags

      # Instantiates a new Rule object.
      #
      # @param [Regexp]		pattern	Regular expression used to match to this rule.
      # @param [Proc]		action	Token producing action associated with this rule.
      # @param [Symbol]		state	State in which this rule is active.
      # @param [Array<Symbol>]	flags	Flags that must be enabled for this rule to match.

      def initialize(@pattern : Regex, @state : Symbol, @flags : Array(Symbol),
                    &@action : Proc(Regex::MatchData, String, CLTK::Lexer::Environment, BlockReturn))
      end
    end
  end
end
