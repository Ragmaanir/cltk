# Description:	This file contains the base class for lexers that use RLTK.

############
# Requires #
############

# Standard Library
require "string_scanner"

# Ruby Language Toolkit
require "./token"

#######################
# Classes and Modules #
#######################

module CLTK

  # A LexingError exception is raised when an input stream contains a
  # substring that isn't matched by any of a lexer's rules.
  class LexingError < Exception
    # @return [Integer]
    getter :stream_offset

    # @return [Integer]
    getter :line_number

    # @return [Integer]
    getter :line_offset

    # @return [String]
    getter :remainder

    # @param [Integer]	stream_offset	Offset from begnning of string.
    # @param [Integer]	line_number	Number of newlines encountered so far.
    # @param [Integer]	line_offset	Offset from beginning of line.
    # @param [String]	remainder		Rest of the string that couldn't be lexed.
    def initialize(@stream_offset, @line_number, @line_offset, @remainder)
      super(message)
      @backtrace = [] of String
    end

    # @return [String] String representation of the error.
    def to_s
      "#{super()}: #{@remainder}"
    end
  end

  # The Lexer class may be sub-classed to produce new lexers.  These lexers
  # have a lot of features, and are described in the main documentation.
  class Lexer

    # @return [Environment] Environment used by an instantiated lexer.
    getter :env

    #################
    # Class Methods #
    #################


    # @return [Symbol] State in which the lexer starts.
    getter :start_state
    def self.start_state
      @@start_state
    end

    def self.setenv(env)
      @@env = env
    end

    # Called when the Lexer class is sub-classed, it installes
    # necessary instance class variables.
    #
    # @return [void]
    macro inherited
      @@match_type	= :longest
      @@rules		= {} of Symbol => Array(Rule)
      @@rules.not_nil![:default] = [] of Rule
      @@start_state	= :default
      self.setenv(CLTK::Lexer::Environment)
    end

    # Lex *string*, using *env* as the environment.  This method will
    # return the array of tokens generated by the lexer with a token
    # of type EOS (End of Stream) appended to the end.
    #
    # @param [String]		string	String to be lexed.
    # @param [String]		file_name	File name used for recording token positions.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex(string, file_name = nil, env = @@env.not_nil!.new(@@start_state))
      # Offset from start of stream.
      stream_offset = 0

      # Offset from the start of the line.
      line_offset = 0
      line_number = 1

      # Empty token list.
      tokens = Array(Token).new

      # The scanner.
      scanner = StringScanner.new(string)

      # Start scanning the input string.
      until scanner.eos?
	match = nil
	# If the match_type is set to :longest all of the
	# rules for the current state need to be scanned
	# and the longest match returned.  If the
	# match_type is :first, we only need to scan until
	# we find a match.
	(@@rules as Hash)[env.state].each do |rule|
	  if (rule.flags - env.flags).empty?
	    if txt = scanner.check(rule.pattern)
	      if !match || match.not_nil!.first.size < txt.size
		match = {txt, rule}
		break if @@match_type == :first
	      end
	    end
	  end
	end
	if match
	  rule = match[1]
	  txt = scanner.scan(rule.pattern).not_nil!

	  type, value = env.rule_exec(rule.pattern.match(txt.not_nil!), txt, rule.action)

	  if type
	    pos = StreamPosition.new(stream_offset, line_number, line_offset, txt.size, file_name)
	    tokens << Token.new(type, value, pos)
	  end

	  # Advance our stat counters.
	  stream_offset += txt.size

	  if (newlines = txt.count("\n")) > 0
	    line_number += newlines
	    line_offset = txt.split("\n").last.size
#	    line_offset = txt.rpartition("\n").last.size
	  else
	    line_offset += txt.size()
	  end
	else
	  error = LexingError.new(stream_offset, line_number, line_offset, scanner.rest)
          raise error
	end
      end

      return tokens << Token.new(:EOS)
    end

    # A wrapper function that calls {Lexer.lex} on the contents of a
    # file.
    #
    # @param [String]		file_name	File to be lexed.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex_file(file_name, env = (@@env).new(@start_state))
      File.open(file_name, 'r') { |f| self.lex(f.read, file_name, env) }
    end

    # Used to tell a lexer to use the first match found instead
    # of the longest match found.
    #
    # @return [void]
    def self.match_first
      @@match_type = :first
    end

    # This method is used to define a new lexing rule.  The
    # first argument is the regular expression used to match
    # substrings of the input.  The second argument is the state
    # to which the rule belongs.  Flags that need to be set for
    # the rule to be considered are specified by the third
    # argument.  The last argument is a block that returns a
    # type and value to be used in constructing a Token. If no
    # block is specified the matched substring will be
    # discarded and lexing will continue.
    #
    # @param [Regexp, String]	pattern	Pattern for matching text.
    # @param [Symbol]			state	State in which this rule is active.
    # @param [Array<Symbol>]		flags	Flags which must be set for rule to be active.
    # @param [Proc]			action	Proc object that produces Tokens.
    #
    # @return [void]

    alias BlockReturn = Symbol | Nil | { Symbol, Int32 } | {String, String} | {Symbol, Float64} |{ Symbol, String } | Tuple(Symbol, Array(String))
    def self.rule(pattern, state = :default, flags = [] of String, &action : String, CLTK::Lexer::Environment -> BlockReturn)

      pattern = Regex.new(pattern) if pattern.is_a?(String)
      r = Rule.new(pattern, action, state, flags)

      if state == :ALL && @@rules.is_a?(Hash)
        (@@rules as Hash).keys.each do |key|
          (@@rules as Hash)[key as Symbol] = @@rules.not_nil![key] << r
        end
      end
      if @@rules
        rules = (@@rules as Hash(Symbol, Array(Rule)))
        if rules.has_key? state
          (rules[state] as Array(Rule)) << r
        else
          rules[state] = [r]
        end
      end
    end

    def self.rule(pattern, state = :default, flags = [] of String)
      # If no action is given we will set it to an empty
      # action.

      action = ->(txt: String, env: CLTK::Lexer::Environment) {}
      pattern = Regex.new(pattern) if pattern.is_a?(String)

      r = Rule.new(pattern, action, state, flags)

      if state == :ALL && @@rules.is_a?(Hash)
        (@@rules as Hash).keys.each do |key|
          (@@rules as Hash)[key as Symbol] = @@rules.not_nil![key] << r
        end
      end
      if @@rules
        rules = (@@rules as Hash(Symbol, Array(Rule)))
        if rules.has_key? state
          (rules[state] as Array(Rule)) << r
        else
          rules[state] = [r]
        end
      end
    end


    # Changes the starting state of the lexer.
    #
    # @param [Symbol] state Starting state for this lexer.
    #
    # @return [void]
    def self.start(state)
      @@start_state = state
    end

    # All actions passed to LexerCore.rule are evaluated inside an
    # instance of the Environment class or its subclass (which must have
    # the same name).  This class provides functions for manipulating
    # lexer state and flags.
    # see http://carc.in/#/r/1i9
    class Environment
      # @return [Array<Symbol>] Flags currently set in this environment.
      getter :flags

      # @return [Match] Match object generated by a rule's regular expression.
      property :match

      # Instantiates a new Environment object.
      #
      # @param [Symbol]	start_state	Lexer's start state.
      # @param [Match]	match		Match object for matching text.
      def initialize(start_state, match = nil)
        @state	= [start_state]
        @match	= match
        @flags	= [] of Symbol
      end

      # This function will instance_exec a block for a rule after
      # setting the match value.
      #
      # @param [Match]	match	Match object for matching text.
      # @param [String]	txt		Text of matching string.
      # @param [Proc]	block	Block for matched rule.
      def rule_exec(match, txt, block)
        self.match = match
        res = block.call(txt, self)
        if res.is_a? Void
          {nil, nil}
        elsif res.is_a? Tuple
          res
        else
          {res, nil}
        end
      end

      # Pops a state from the state stack.
      #
      # @return [void]
      def pop_state
        @state.pop

        nil
      end

      # Pushes a new state onto the state stack.
      #
      # @return [void]
      def push_state(state)
        @state << state

        nil
      end

      # Sets the value on the top of the state stack.
      #
      # @param [Symbol] state New state for the lexing environment.
      #
      # @return [void]
      def set_state(state)
        @state[-1] = state

        nil
      end

      # @return [Symbol] Current state of the lexing environment.
      def state
        @state.last
      end

      # Sets a flag in the current environment.
      #
      # @param [Symbol] flag Flag to set as enabled.
      #
      # @return [void]
      def set_flag(flag)
        unless @flags.includes?(flag)
	  @flags << flag
        end

        nil
      end

      # Unsets a flag in the current environment.
      #
      # @param [Symbol] flag Flag to unset.
      #
      # @return [void]
      def unset_flag(flag)
        @flags.delete(flag)

        nil
      end

      # Unsets all flags in the current environment.
      #
      # @return [void]
      def clear_flags
        @flags = Array.new

        nil
      end
    end

    ####################
    # Instance Methods #
    ####################

    # Instantiates a new lexer and creates an environment to be
    # used for subsequent calls.
    def initialize
      @env = (@@env || Environment).new(self.class.start_state)
    end

    # Lexes a string using the encapsulated environment.
    #
    # @param [String] string		String to be lexed.
    # @param [String] file_name	File name used for Token positions.
    #
    # @return [Array<Token>]
    def lex(string, file_name = nil)
      self.class.lex(string, file_name, @env)
    end

    # Lexes a file using the encapsulated environment.
    #
    # @param [String] file_name File to be lexed.
    #
    # @return [Array<Token>]
    def lex_file(file_name)
      self.class.lex_file(file_name, @env)
    end

    # The Rule class is used simply for data encapsulation.
    class Rule
      # @return [Proc] Token producting action to be taken when this rule is matched.
      getter :action

      # @return [Regexp] Regular expression for matching this rule.
      getter :pattern

      # @return [Array<Symbol>] Flags currently set in this lexing environment.
      getter :flags

      # Instantiates a new Rule object.
      #
      # @param [Regexp]		pattern	Regular expression used to match to this rule.
      # @param [Proc]		action	Token producing action associated with this rule.
      # @param [Symbol]		state	State in which this rule is active.
      # @param [Array<Symbol>]	flags	Flags that must be enabled for this rule to match.
      def initialize(@pattern, @action, @state, @flags)
      end
    end
  end
end
