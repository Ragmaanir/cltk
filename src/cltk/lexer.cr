# Description:	This file contains the base class for lexers that use CLTK.

############
# Requires #
############

# Standard Library
require "string_scanner"

# Crystal Language Toolkit
require "./token"
require "./lexer/environment"
require "./lexer/rule"
#######################
# Classes and Modules #
#######################

def yield_with(env)
  with env yield
end

module CLTK

  # A LexingError exception is raised when an input stream contains a
  # substring that isn't matched by any of a lexer's rules.
  class LexingError < Exception
    # @return [Integer]
    getter :stream_offset

    # @return [Integer]
    getter :line_number

    # @return [Integer]
    getter :line_offset

    # @return [String]
    getter :remainder

    # @param [Integer]	stream_offset	Offset from begnning of string.
    # @param [Integer]	line_number	Number of newlines encountered so far.
    # @param [Integer]	line_offset	Offset from beginning of line.
    # @param [String]	remainder		Rest of the string that couldn't be lexed.
    def initialize(@stream_offset : Int32, @line_number : Int32, @line_offset : Int32, @remainder : String)
      super(message)
      @backtrace = [] of String
    end

    # @return [String] String representation of the error.
    def to_s
      "#{super()}: #{@remainder}"
    end
  end

  # The Lexer class may be sub-classed to produce new lexers.  These lexers
  # have a lot of features, and are described in the main documentation.
  abstract class Lexer

    # @return [Environment] Environment used by an instantiated lexer.
    getter :env

    #################
    # Class Methods #
    #################

    # @return [Symbol] State in which the lexer starts.
    getter :start_state

    def self.start_state
      @@start_state
    end

    @@match_type	= :longest
    @@start_state	= :default

    @@rules : Hash(Symbol, Array(Rule) ) = {} of Symbol => Array(Rule)

    # Called when the Lexer class is sub-classed, it installes
    # necessary instance class variables.
    #
    # @return [void]
    macro inherited
      @env : {{@type.id}}::Environment
      @@env = {{@type.id}}::Environment

      ####################
      # Instance Methods #
      ####################

      def initialize
        @env = {{@type.id}}::Environment.new(@@start_state)
      end

      # Lexes a string using the encapsulated environment.
      #
      # @param [String] string		String to be lexed.
      # @param [String] file_name	File name used for Token positions.
      #
      # @return [Array<Token>]
      def lex(string, file_name = nil)
        self.class.lex(string, file_name, @env)
      end

      # Lexes a file using the encapsulated environment.
      #
      # @param [String] file_name File to be lexed.
      #
      # @return [Array<Token>]
      def lex_file(file_name)
        self.class.lex_file(file_name, @env)
      end

      @@rules		= {} of Symbol => Array(Rule)
      @@rules[:default] = [] of Rule
    end

    # Lex *string*, using *env* as the environment.  This method will
    # return the array of tokens generated by the lexer with a token
    # of type EOS (End of Stream) appended to the end.
    #
    # @param [String]		string	String to be lexed.
    # @param [String]		file_name	File name used for recording token positions.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex(string, file_name = nil, env = @@env.new(@@start_state))
      # Offset from start of stream.
      stream_offset = 0

      # Offset from the start of the line.
      line_offset = 0
      line_number = 1

      # Empty token list.
      tokens = Array(Token).new

      # The scanner.
      scanner = StringScanner.new(string)

      # Start scanning the input string.
      until scanner.eos?
	match = nil
	# If the match_type is set to :longest all of the
	# rules for the current state need to be scanned
	# and the longest match returned.  If the
	# match_type is :first, we only need to scan until
	# we find a match.
	@@rules[env.state].each do |rule|
	  if (rule.flags - env.flags).empty?
	    if txt = scanner.check(rule.pattern)
	      if !match || match.first.size < txt.size
		match = {txt, rule}
		break if @@match_type == :first
	      end
	    end
	  end
	end
	if match
	  rule = match[1]
	  txt = scanner.scan(rule.pattern).not_nil!

          type, value = rule.action.call(rule.pattern.match(txt).not_nil!, txt, env)

	  if type
	    pos = StreamPosition.new(stream_offset, line_number, line_offset, txt.size, file_name)
	    tokens << Token.new(type, value, pos)
	  end

	  # Advance our stat counters.
	  stream_offset += txt.size

	  if (newlines = txt.count("\n")) > 0
	    line_number += newlines
	    line_offset = txt.split("\n").last.size
	  else
	    line_offset += txt.size()
	  end
	else
          raise LexingError.new(stream_offset, line_number, line_offset, scanner.rest)
	end
      end

      return tokens << Token.new(:EOS)
    end

    # A wrapper function that calls {Lexer.lex} on the contents of a
    # file.
    #
    # @param [String]		file_name	File to be lexed.
    # @param [Environment]	env		Lexing environment.
    #
    # @return [Array<Token>]
    def self.lex_file(file_name, env = @@env.new(@start_state))
      File.open(file_name, 'r') { |f| self.lex(f.read, file_name, env) }
    end

    # Used to tell a lexer to use the first match found instead
    # of the longest match found.
    #
    # @return [void]
    def self.match_first
      @@match_type = :first
    end

    # This method is used to define a new lexing rule.  The
    # first argument is the regular expression used to match
    # substrings of the input.  The second argument is the state
    # to which the rule belongs.  Flags that need to be set for
    # the rule to be considered are specified by the third
    # argument.  The last argument is a block that returns a
    # type and value to be used in constructing a Token. If no
    # block is specified the matched substring will be
    # discarded and lexing will continue.
    #
    # @param [Regexp, String]	pattern	Pattern for matching text.
    # @param [Symbol]			state	State in which this rule is active.
    # @param [Array<Symbol>]		flags	Flags which must be set for rule to be active.
    # @param [Proc]			action	Proc object that produces Tokens.
    #
    # @return [void]

    alias BlockReturn = { Symbol, Nil } | {String, String} | { Nil, Nil } | { Symbol, Int32 } | {Symbol, Float64} | { Symbol, String } | { Symbol, Array(String) }

    macro rule(pattern, state = :default, flags = [] of Symbol, &action: _ -> _)
      {{pattern}}.tap do |pattern|
        Rule.new(
          (pattern.is_a?(String)) ? Regex.new(pattern) : pattern,
          {{state}},
          {{flags}}
        ) do |%match, %txt, %env|
            {% if action  %}
              {% if action.args.first %}
                {{action.args.first.id}} = %txt
              {% end %}
            %env.match = %match
            %res = yield_with(%env.as({{@type}}::Environment)) do
              {{action.body}}
            end

            if %res.is_a? Void
              {nil, nil}
            elsif %res.is_a? Tuple
              %res
            else
              Tuple.new(%res, nil)
            end
            {% else %}
              {nil, nil}
            {% end %}
        end.tap do |rule|
          if {{state}} == :ALL
            @@rules.each_key { |k| @@rules[k] << rule }
          elsif @@rules[{{state}}]?
            @@rules[{{state}}] << rule
          else
            @@rules[{{state}}] = [ rule ]
          end
        end
      end
    end

    def self.rule(pattern, state = :default, flags = [] of String)
      self.rule(pattern, state, flags) do
        {nil, nil}
      end
    end

    # Changes the starting state of the lexer.
    #
    # @param [Symbol] state Starting state for this lexer.
    #
    # @return [void]
    def self.start(state)
      @@start_state = state
    end

  end
end
